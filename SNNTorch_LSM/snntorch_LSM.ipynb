{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snntorch in /usr/local/lib/python3.12/dist-packages (0.9.4)\n",
      "Requirement already satisfied: tonic in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.12/dist-packages (from tonic) (1.26.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from tonic) (3.15.1)\n",
      "Requirement already satisfied: importRosbag>=1.0.4 in /usr/local/lib/python3.12/dist-packages (from tonic) (1.0.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from tonic) (1.16.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from tonic) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from tonic) (4.15.0)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from tonic) (0.11.0)\n",
      "Requirement already satisfied: pbr in /usr/local/lib/python3.12/dist-packages (from tonic) (7.0.3)\n",
      "Requirement already satisfied: expelliarmus in /usr/local/lib/python3.12/dist-packages (from tonic) (1.1.12)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from importRosbag>=1.0.4->tonic) (75.2.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->tonic) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->tonic) (0.60.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->tonic) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->tonic) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->tonic) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa->tonic) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->tonic) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->tonic) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->tonic) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->tonic) (1.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa->tonic) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->tonic) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->tonic) (4.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->tonic) (2.32.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->tonic) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa->tonic) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->tonic) (2.23)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->tonic) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install snntorch\n",
    "!pip install tonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./drive/MyDrive/Colab\\ Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/1afc103f-8799-464a-a214-81bb9b1f9337 to ./data/NMNIST/train.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5eb75b147904f26a8f5bcb637c30e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011893601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/NMNIST/train.zip to ./data/NMNIST\n",
      "Downloading https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/a99d0fee-a95b-4231-ad22-988fdb0a2411 to ./data/NMNIST/test.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8d55c72c6c47119eca1a443c9e6b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169674850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/NMNIST/test.zip to ./data/NMNIST\n",
      "cuda\n",
      "torch.Size([311, 256, 2312])\n",
      "train batches completed:  24\n",
      "train batches completed:  49\n",
      "train batches completed:  74\n",
      "train batches completed:  99\n",
      "train batches completed:  124\n",
      "train batches completed:  149\n",
      "train batches completed:  174\n",
      "train batches completed:  199\n",
      "train batches completed:  224\n",
      "running time of training epoch:  2001.3650555610657 seconds\n",
      "test batches completed:  24\n",
      "(60000, 1000)\n",
      "(10000, 1000)\n",
      "(60000, 2312)\n",
      "(10000, 2312)\n",
      "mean in spiking (train) :  0.004515078\n",
      "mean in spiking (test) :  0.0045421165\n",
      "mean LSM spiking (train) :  0.55149764\n",
      "mean LSM spiking (test) :  0.5517097\n",
      "training linear model:\n",
      "test score = 0.9493\n"
     ]
    }
   ],
   "source": [
    "import tonic\n",
    "from tonic import DiskCachedDataset\n",
    "import tonic.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import time\n",
    "\n",
    "#from lsm_weight_definitions import initWeights1\n",
    "#from lsm_models import LSM\n",
    "import lsm_weight_definitions as lsm_wts\n",
    "import lsm_models\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Load dataset (Using NMNIST here)\n",
    "    sensor_size = tonic.datasets.NMNIST.sensor_size\n",
    "    frame_transform = transforms.Compose([transforms.Denoise(filter_time=3000),\n",
    "                                          transforms.ToFrame(sensor_size=sensor_size,time_window=1000)])\n",
    "\n",
    "    trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n",
    "    testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)\n",
    "\n",
    "    cached_trainset = DiskCachedDataset(trainset, cache_path='./cache/nmnist/train')\n",
    "    cached_testset = DiskCachedDataset(testset, cache_path='./cache/nmnist/test')\n",
    "\n",
    "    batch_size = 256\n",
    "    trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
    "    testloader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n",
    "\n",
    "    #Set device\n",
    "    #device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    data, targets = next(iter(trainloader))\n",
    "    flat_data = torch.reshape(data, (data.shape[0], data.shape[1], -1))\n",
    "    print(flat_data.shape)\n",
    "\n",
    "    in_sz = flat_data.shape[-1]\n",
    "\n",
    "    #Set neuron parameters\n",
    "    tauV = 16.0\n",
    "    tauI = 16.0\n",
    "    th = 20\n",
    "    curr_prefac = np.float32(1/tauI)\n",
    "    alpha = np.float32(np.exp(-1/tauI))\n",
    "    beta = np.float32(1 - 1/tauV)\n",
    "\n",
    "    Win, Wlsm = lsm_wts.initWeights1(27, 2, 0.15, in_sz)\n",
    "    N = Wlsm.shape[0]\n",
    "    lsm_net = lsm_models.LSM(N, in_sz, np.float32(curr_prefac*Win), np.float32(curr_prefac*Wlsm), alpha=alpha, beta=beta, th=th).to(device)\n",
    "    lsm_net.eval()\n",
    "    #Run with no_grad for LSM\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for i, (data, targets) in enumerate(iter(trainloader)):\n",
    "            if i%25 == 24:\n",
    "                print(\"train batches completed: \", i)\n",
    "            flat_data = torch.reshape(data, (data.shape[0], data.shape[1], -1)).to(device)\n",
    "            spk_rec = lsm_net(flat_data)\n",
    "            lsm_out = torch.mean(spk_rec, dim=0)\n",
    "            if i==0:\n",
    "                in_train = torch.mean(flat_data, dim=0).cpu().numpy()\n",
    "                lsm_out_train = lsm_out.cpu().numpy()\n",
    "                lsm_label_train = np.int32(targets.numpy())\n",
    "            else:\n",
    "                in_train = np.concatenate((in_train, torch.mean(flat_data, dim=0).cpu().numpy()), axis=0)\n",
    "                lsm_out_train = np.concatenate((lsm_out_train, lsm_out.cpu().numpy()), axis=0)\n",
    "                lsm_label_train = np.concatenate((lsm_label_train, np.int32(targets.numpy())), axis=0)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\"running time of training epoch: \", end_time - start_time, \"seconds\")\n",
    "\n",
    "        for i, (data, targets) in enumerate(iter(testloader)):\n",
    "            if i%25 == 24:\n",
    "                print(\"test batches completed: \", i)\n",
    "            flat_data = torch.reshape(data, (data.shape[0], data.shape[1], -1)).to(device)\n",
    "            lsm_net.eval()\n",
    "            spk_rec = lsm_net(flat_data)\n",
    "            lsm_out = torch.mean(spk_rec, dim=0)\n",
    "            if i==0:\n",
    "                in_test = torch.mean(flat_data, dim=0).cpu().numpy()\n",
    "                lsm_out_test = lsm_out.cpu().numpy()\n",
    "                lsm_label_test = np.int32(targets.numpy())\n",
    "            else:\n",
    "                in_test = np.concatenate((in_test, torch.mean(flat_data, dim=0).cpu().numpy()), axis=0)\n",
    "                lsm_out_test = np.concatenate((lsm_out_test, lsm_out.cpu().numpy()), axis=0)\n",
    "                lsm_label_test = np.concatenate((lsm_label_test, np.int32(targets.numpy())), axis=0)\n",
    "\n",
    "    print(lsm_out_train.shape)\n",
    "    print(lsm_out_test.shape)\n",
    "\n",
    "    print(in_train.shape)\n",
    "    print(in_test.shape)\n",
    "\n",
    "    print(\"mean in spiking (train) : \", np.mean(in_train))\n",
    "    print(\"mean in spiking (test) : \", np.mean(in_test))\n",
    "\n",
    "    print(\"mean LSM spiking (train) : \", np.mean(lsm_out_train))\n",
    "    print(\"mean LSM spiking (test) : \", np.mean(lsm_out_test))\n",
    "\n",
    "    print(\"training linear model:\")\n",
    "    clf = linear_model.SGDClassifier(max_iter=10000, tol=1e-6)\n",
    "    clf.fit(lsm_out_train, lsm_label_train)\n",
    "\n",
    "    score = clf.score(lsm_out_test, lsm_label_test)\n",
    "    print(\"test score = \" + str(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
